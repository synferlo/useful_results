
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{indentfirst}
\setlength{\parindent}{0em}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{setspace}
\doublespacing
\usepackage[flushleft]{threeparttable}
\usepackage{booktabs,caption}
\usepackage{float}
\usepackage{graphicx}

\usepackage{import}
\usepackage{xifthen}
\usepackage{pdfpages}
\usepackage{transparent}

\newcommand{\incfig}[1]{%
\def\svgwidth{\columnwidth}
\import{./figures/}{#1.pdf_tex}
}




\title{Useful Results in Econ}
\author{Synferlo}
\date{Dec. 15, 2020}


\begin{document}
\maketitle
\newpage


\section{SSOC for utility/profit maximization}

\subsection{Utility maximization}

Given $ U = U(X_1, X_2) $, quasi-concave, utility maximization requires
\begin{align}
				du  &= u_1dx_1 + u_2dx_2 = 0\\
				d^{2}u  &= (u_{11}dx_1 + u_{12}dx_2)dx_1 + (u_{21}dx_1 + u_{22}dx_2)dx_2
				< 0.
\end{align}
Therefore, we can rewrite equation (2)
\begin{align}
				&u_{11}dx_1^{2} + u_{12}dx_1dx_2 + u_{21}dx_1dx_2 + u_{22}dx_2^{2}  <0\\
				&u_{11}(dx_1^{2} + \frac{2u_{12}}{u_{11}}dx_1dx_2) + u_{22}dx_2^{2} <0\\
				&u_{11}\left(dx_1 + \frac{2u_{12}}{u_{11}}dx_1dx_2 + (\frac{u_{12}}{u_{11}}dx_2
				)^{2} - (\frac{u_{12}}{u_{11}}dx_2)^{2}\right) + u_{22}dx_2^{2} <0\\
				&u_{11}(dx_1 + \frac{u_{12}}{u_{11}}dx_2)^{2} + dx_2^{2}
				(u_{22} - \frac{u_{12}^{2}}{u_{11}}) < 0
\end{align}

where $ u_{11} < 0 $, $ (dx_1 + \frac{u_{12}}{u_{11}}dx_2)^{2} $ and $ dx_2^{2} $
are positive. Hence,
\begin{align}
				u_{22} - \frac{u_{12}^{2}}{u_{11}} < 0\\
				u_{11}u_{22} - u_{12}^{2} > 0.
\end{align}



\subsection{Profit maximization}

Given $ \pi = pf - w_1x_1 - w_2x_2 $, where $ f(\cdot ) $ stands for the 
production function, $ f = f(x_1, x_2) $.

\begin{align}
				d \pi  &= (pf_1 - w_1)dx_1 + (pf_2 - w_2)dx_2 =0\\
				d^{2} \pi  &= p(f_{11}dx_1 + f_{12}dx_2)dx_1 + p(f_{21}dx_1 + p_{22}dx_2)
				dx_2 <0.
\end{align}

Therefore, we can rewrite equation (10) as the following,
\begin{align}
				&p(f_{11}dx_1 + f_{12}dx_2)dx_1 + p(f_{21}dx_1 + f_{22}dx_2)dx_2 <0\\
				&f_{11}dx_1^{2} + f_{12}dx_1dx_2 + f_{12}dx_1dx_2 + f_{22}dx_2^{2} <0\\
				&f_{11}\left( dx_1 + \frac{f_{12}}{f_{11}}dx_2 \right) ^{2} + 
				\left( f_{22} - \frac{f_{12}^{2}}{f_{11}} \right) dx_2^{2} < 0.
\end{align}
Hence, we have similar results as in utility maximization problem,
\begin{align}
				&f_{22} - \frac{f_{12}^{2}}{f_{11}} <0\\
				&f_{11}f_{22} - f_{12}^{2} > 0.
\end{align}




\section{Cramer's Rule and Inferior goods}

\subsection{Cramer's Rule}

Given a comb of three equations,
\begin{equation}
				\begin{cases}
					a_{11}x_1 + a_{12}x_2 + a_{13}x_3 = b_1\\
					a_{21}x_1 + a_{22}x_2 + a_{23}x_3 = b_2\\
					a_{31}x_1 + a_{32}x_2 + a_{33}x_3 = b_3\\
				\end{cases}
\end{equation}

we can rewrite in matrix form, i.e., $ \bm{A}\bm{x} = \bm{b} $,
where
\begin{equation}
				\bm{A} = 
				\begin{bmatrix}
				a_{11} & a_{12} & a_{13} \\
				a_{21} & a_{22} & a_{23} \\
				a_{31} & a_{32} & a_{33} 
				\end{bmatrix}, \quad
				\bm{x} = 
				\begin{bmatrix}
				x_1 \\
				x_2 \\
				x_3
				\end{bmatrix}, \quad
				\bm{b} = 
				\begin{bmatrix}
				b_1 \\
				b_2 \\
				b_3
				\end{bmatrix}.
\end{equation}

Therefore, we can solve $ x $ with the following rule,
\begin{equation}
				x_1 = 
				\frac{
								\begin{vmatrix}
								b_1		&a_{12}		&a_{13} \\
								b_2		&a_{22}		&a_{23} \\
								b_3		&a_{32}		&a_{33} \\
								\end{vmatrix}
				}{det(A)}, \quad
				x_2 = 
				\frac{
								\begin{vmatrix}
								a_{11}		&b_1		&a_{13} \\
								a_{21}		&b_2		&a_{23} \\
								a_{31}		&b_3		&a_{33} \\
								\end{vmatrix}
				}{det(A)}, \quad
				x_3 = 
				\frac{
								\begin{vmatrix}
								a_{11}		&a_{12}		&b_1 \\
								a_{21}		&a_{22}		&b_2 \\
								a_{31}		&a_{32}		&b_3 \\
								\end{vmatrix}
				}{det(A)}
\end{equation}






\subsection{Inferior good}

Consider a two--good model,

\begin{equation}
				\mathscr{L} = U(x_1, x_2) + \lambda (M - p_1x_1 - p_2x_2).
\end{equation}

Then we can write the first order conditions (FOCs) as the following,
\begin{align}
				\frac{\partial \mathscr{L} }{\partial x_1 } &= u_1 - \lambda p_1 = 0\\
				\frac{\partial \mathscr{L} }{\partial x_2 } &= u_2 - \lambda p_2 = 0\\
				\frac{\partial \mathscr{L} }{\partial \lambda  } &=
				M - p_1x_1 - p_2x_2 = 0
\end{align}
where $ u_{i} $ stands for the marginal utility of good $ i $, i.e., 
$ u_{i} = \frac{\partial U }{\partial x_{i} } $.

Now, we can obtain boarder Hessian matrix by differentiating equation (20) --
(22) with respect to income, M. 
\begin{align}
				\frac{\partial  }{\partial M }\frac{\partial \mathscr{L} }{\partial x_1 }
				 &= u_{11}\frac{\partial x_1 }{\partial M  } + u_{12}\frac{\partial 
				 x_2}{\partial M  } - p_1 \frac{\partial \lambda  }{\partial M } = 0\\
				\frac{\partial  }{\partial M }\frac{\partial \mathscr{L} }{\partial x_2 }
				 &= u_{21}\frac{\partial x_1 }{\partial M  } + u_{22}\frac{\partial x_2
				 }{\partial M} - p_2 \frac{\partial \lambda  }{\partial M } = 0\\
				\frac{\partial  }{\partial M }\frac{\partial \mathscr{L} }{\partial
				\lambda } &= -p_1 \frac{\partial x_1 }{\partial M  } - p_2
				\frac{\partial x_2 }{\partial M  } + 0 = -1
\end{align}
\begin{equation}
				\begin{bmatrix}
				u_{11} & u_{12} &-p_1 \\
				u_{21} & u_{22} &-p_2 \\
				-p_1 & -p_2 &0 \\
				\end{bmatrix}\cdot 
				\begin{bmatrix}
				\frac{\partial x_1 }{\partial M  }\\
				\frac{\partial x_2 }{\partial M  }\\
				\frac{\partial \lambda  }{\partial M  }\\
				\end{bmatrix}=
				\begin{bmatrix}
				0\\
				0\\
				-1
				\end{bmatrix}
\end{equation}

Suppose we want to verify if $ x_1 $ is inferior,
\begin{equation}
				\frac{\partial x_1 }{\partial M  } = 
				\frac{
				\begin{vmatrix}
								0 &u_{12} &-p_1\\
								0 &u_{22} &-p_2\\
								-1 &-p_2 &0\\
				\end{vmatrix}
				}{det(H)}, \text{ where }
				H = 
				\begin{bmatrix}
				u_{11} & u_{12} &-p_1 \\
				u_{21} & u_{22} &-p_2 \\
				-p_1 & -p_2 &0 \\
				\end{bmatrix}.
\end{equation}

In two-good case, $ det(H) > 0 $, so we only need to check the sign of 
the numerator.

\begin{equation}
				\begin{vmatrix}
								0 &u_{12} &-p_1\\
								0 &u_{22} &-p_2\\
								-1 &-p_2 &0\\
				\end{vmatrix} = 
				(-1)^{3+1}\cdot (-1)\cdot 
				\begin{vmatrix}
								u_{12} &-p_1\\
								u_{22} &-p_2\\
				\end{vmatrix}
				= u_{12}p_2 - u_{22}p_1.
\end{equation}

Hence,
\begin{equation}
				\frac{\partial x_1 }{\partial M  } = \frac{u_{12}p_2 - u_{22}p_1}{
				det(H)},
\end{equation}
where prices are non-negative, $ u_{22} $ is negative. We do not know the sign
of $ u_{12} $.

If $ u_{12} $ is known, we know the sign of $ \frac{\partial x_1 }{\partial M} $.
Good $ x_1 $ is inferior if the partial derivative w.r.t M is negative.



\section{Hessian and optimization}

We are going to test if Hessian matrix shows the same result as the convexity
of the indifference curve (IDC) in two--good optimization problem, i.e., 
the proof of $ det(H) > 0 $ is also the proof of convexity of IDC.

MRS stands for the slope of the IDC, 
\begin{equation}
				MRS_{12} = \frac{dx_2}{dx_1} = -\frac{u_1}{u_2}
\end{equation}

If IDC is convex, $ \frac{dMRS}{dx_1} > 0 $. To show this,
\begin{align}
				dMRS &= d(-\frac{u_1}{u_2})\\
						 &= -\frac{1}{u_2^{2}}(u_2(u_{11}dx_1 + u_{12}dx_2) - 
						 u_1(u_{21}dx_1 + u_{22}dx_2))\\
				\frac{dMRS}{dx_1} &= -\frac{1}{u_2^{2}}\left( 
				u_2(u_{11} + u_{12}\frac{dx_2}{dx_1}) - u_1(u_{21} + u_{22}\frac{dx_2}{
				dx_1})\right) \\
				 &= -\frac{1}{u_2^{3}}\left( 
				 u_2^{2}u_{11} + u_2^{2}u_{12}\frac{dx_2}{dx_1} - u_1u_2u_{12}
				 - u_1u_2u_{22}\frac{dx_2}{dx_1}\right) .
\end{align}
Recall, $ \frac{dx_2}{dx_1} = -\frac{u_1}{u_2} $. We can substitute it into
equation (34),
\begin{align}
				\frac{dMRS}{dx_1} &= 
				-\frac{1}{u_2^{3}}\left( u_2^{2}u_{11} + u_2^{2}u_{12}(-\frac{u_1}{u_2})
				 - u_1u_2u_{12} - u_1u_2u_{22}(-\frac{u_1}{u_2})\right) \\
				 &= -\frac{1}{u_2^{3}}(u_2^{2}u_{11} - 2u_1u_2u_{12} + u_1^{2}u_{22})
				 >0.
\end{align}


Now, let's see if Hessian matrix can tell us the same thing.

Given boarder Hessian H, where
\begin{equation}
				H = 
				\begin{bmatrix}
				u_{11} &u_{12} &u_1 \\
				u_{21} &u_{22} &u_2 \\
				u_{1} &u_{2} &0 \\
				\end{bmatrix}.
\end{equation}


\begin{align}
				det(H) &= (-1)^{1 + 1}\cdot u_{11}\cdot 
				\begin{vmatrix}
								u_{22} & u_2\\
								u_2    &0
				\end{vmatrix}
				 + (-1)^{1 + 2}\cdot u_{12}\cdot 
				\begin{vmatrix}
				u_{12} &u_2\\
				u_1 &0\\
				\end{vmatrix}
				 + (-1)^{1 + 3}\cdot u_1\cdot 
				 \begin{vmatrix}
								 u_{12} &u_{22}\\
								 u_1 & u_2
				 \end{vmatrix}\\
							 &= -u_2^{2}u_{11} + u_1u_2u_{12} + u_1(u_2u_{12} - u_1u_{22})\\
							 &= -(u_2^{2}u_{11} - 2u_1u_2u_{12} + u_1^{2}u_{22}) > 0
\end{align}

Clearly, equation (36) and (40) are exactly the same thing.




\section{Conditional factor demand and Shephard's Lemma}

\subsection{An example for Shephard's Lemma}

Consider Cobb-Douglas (CD) production function, $ y = x_1^{\alpha }x_2^{\beta } $.
We can write the cost minimization problem as the following,
\begin{equation}
				\mathscr{L} = w_1x_1 + w_2x_2 + \lambda (y_0 - x_1^{\alpha }x_2^{\beta })
\end{equation}
where $ w_{i} $ stands for the price of factor $ i $, and we have a given
output level $ y_0 $. Write down the FOCs,
\begin{align}
				\frac{\partial \mathscr{L} }{\partial x_1 }
				 &= w_1 - \lambda \alpha x_1^{\alpha  - 1}x_2^{\beta } = 0\\
				\frac{\partial \mathscr{L} }{\partial x_2 }
				 &= w_2 - \lambda \beta x_1^{\alpha }x_2^{\beta  - 1} = 0\\
				\frac{\partial \mathscr{L} }{\partial \lambda  }
				 &= y_0 - x_1^{\alpha }x_2^{\beta } = 0
\end{align}

Divide (42) by (43), we have $ x_2 = \frac{w_1}{w_2} \frac{\beta}{\alpha }x_1 $.
Substitute back to (44)
\begin{align}
				y_0 &= x_1^{\alpha }(\frac{w_1}{w_2})^{\beta }(\frac{\beta}{\alpha })^{
				\beta }x_1^{\beta }\\
				x_1^{*} &= y_0^{\frac{1}{\alpha  + \beta }}(\frac{w_2}{w_1})^{
				\frac{\beta}{\alpha  + \beta }}(\frac{\alpha}{\beta })^{\frac{\beta}{
        \alpha + \beta }}\\
				x_2^{*} &= y_0^{\frac{1}{\alpha  + \beta }}
				(\frac{w_1}{w_2})^{\frac{\alpha}{\alpha  + \beta }}
				(\frac{\beta}{\alpha})^{\frac{\alpha}{\alpha + \beta }}
\end{align}

Now we can substitute the optimal factor demand back to the cost function,
then we have $ C^{*} = C^{*}(x_1^{*}, x_2^{*}) $,
\begin{align}
				C^{*} &= w_1x_1^{*} + w_2x_2^{*}\\
				 &= \frac{\alpha}{\beta }w_2x_2 + w_2x_2\\
				 &= \frac{\alpha  + \beta }{\beta }w_2x_2\\
				 &= \frac{\alpha  + \beta }{\beta }w_2y_0^{\frac{1}{\alpha  + \beta }}
				(\frac{w_1}{w_2})^{\frac{\alpha}{\alpha  + \beta }}
				(\frac{\beta}{\alpha })^{\frac{\alpha}{\alpha  + \beta }}\\
				\frac{\partial C^{*} }{\partial w_1 }
				 &= y_0^{\frac{1}{\alpha  + \beta }}(\frac{w_2}{w_1})^{\frac{\beta}{
				 \alpha  + \beta }}(\frac{\alpha}{\beta })^{\frac{\beta}{
				 \alpha  + \beta }}\\
				 &= X_1^{*}.
\end{align}






\subsection{Shephard's lemma}


Consider a cost minimization problem in two--factor model,
\begin{equation}
				\mathscr{L} = \sum\limits_{i = 1} ^n w_{i}x_{i} + \lambda 
				(\overline{y} - f(x_1, \cdots, x_{n})),
\end{equation}
where $ f(\cdot ) $ stands for the production function.

We can write down the FOC,
\begin{equation}
				\frac{\partial \mathscr{L} }{\partial x_{i} } = w_{i} - \lambda f_{i}=0,
\end{equation}
where $ f_{i} $ stands for the marginal product of factor $ i $, i.e., 
$ f_{i} = \frac{\partial f }{\partial x_{i} } $.
Then, solving optimal factor demand $ x_{i}^{*} $ is quite straightforward
, $ x_{i}^{*} = x_{i}^{*}(\bm{w},\overline{y}) $.

Plug $ x_{i}^{*} $ back to cost function, we have $ C^{*} = C^{*}(\bm{w},
\overline{y}) $.

Then, take partial derivative w.r.t. $ w_{i} $, 
\begin{equation}
				\frac{\partial C^{*} }{\partial w_{j} } = 
				x_{j}^{*} + \sum\limits_{i = 1} ^n w_{i}\frac{\partial x_{i}^{*} }
				{\partial w_{j} }	
\end{equation}
From FOC, we know $ w_{i} = \lambda f_{i} $, plug into equation (56),
\begin{equation}
				\frac{\partial C^{*} }{\partial w_{j} } = 
				x_{j}^{*} + \sum\limits_{i = 1} ^n \lambda f_{i}\frac{\partial x_{i}^{*}
				}{\partial w_{j} }	
\end{equation}
Clearly, 
\begin{equation}
				\frac{\partial f(x_1^{*},\cdots, x_{n}^{*}) }{\partial w_{j} }
				=\sum\limits_{i = 1} ^n f_{i}\frac{\partial x_{i}^{*} }{\partial w_{j} }
				=\frac{\partial \overline{y} }{\partial w_{j} } = 0
\end{equation}
Note, $ \overline{y} $ is constant, so equation (58) = 0.
Hence, the second term in equation (57) is zero, and we have
\begin{equation}
				\frac{\partial C^{*} }{\partial w_{j} } = x_{j}^{*}.
\end{equation}

Shephard's lemma says $ C^{*} $ is differentiable in $ \bm{w} $ at 
$ (\bm{w}^{0}, \overline{y}^{0}) $ with $ \bm{w}^{0} > > 0 $, and
\begin{equation}
				\frac{\partial C^{*} }{\partial w_{j} } = x_{j}^{*} = x_{j}^{H}(
				\bm{w}^{0},\overline{y}^{0}), \quad j = 1, \cdots, n.
\end{equation}


We are going to prove this by invoking Envelope theorem in next section.






\subsection{Envelope theorem and Shephard's lemma}


Consider a cost minimization problem in two--factor model, we obtain optimal
factor demand $ x_{j}^{*} $ and Lagrangian multiplier $ \lambda^{*} $
by solving Lagrangian equation, where $ x_{j}^{*} = x_{j}^{*}(\bm{w},\overline{y}
)  $ and $ \lambda ^{*} = \lambda ^{*}(\bm{w},\overline{y}) $. Plug the optimal
solution back to Lagrangian, 
\begin{equation}
				\mathscr{L}^{*}(\bm{w},\overline{y}) = \sum\limits_{i = 1} ^n w_{i}x_{i}
				^{*} + \lambda ^{*}(\overline{y} - f(\bm{x}^{*}))
\end{equation}
where $ \bm{w} $ and $ \bm{x} $ are vectors, $ \bm{w} = [w_1,w_2,\cdots,w_{n}] $,
$ \bm{x} = [x_1, \cdots, x_{n}] $.

Differentiate Lagrangian w.r.t $ w_1 $,
\begin{align}
				\frac{\partial \mathscr{L}^{*} }{\partial w_1 } &= 
				x_1^{*} + \sum\limits_{i = 1} ^n w_{i}\frac{\partial x_{i}^{*} }
				{\partial w_1 }	 + \lambda ^{*}\left( 
				-\sum\limits_{i = 1} ^n f_{i}\frac{\partial x_{i}^{*} }
        {\partial w_{1} }	\right)  + \frac{\partial \lambda ^{*} }
				{\partial w_1 }(\overline{y} - f(x_1^{*}, \cdots, x_{n}^{*}))\\
				&= x_1^{*} + \left(\sum\limits_{i = 1} ^n (w_{i} -
				\lambda ^{*}f^{*}_{i})\frac{\partial x_{i}^{*} }{\partial w_1 }\right)
				+ \frac{\partial \lambda ^{*} }{\partial w_1 }(\overline{y} - 
				f(x_1^{*},\cdots,x_{n}^{*})).
\end{align}
From FOC, we know $ w_{i} - \lambda f_{i} = 0 $, $ \overline{y} - f(\bm{x}^{*})
=0$, so the last two terms in equation (63) equals to zero.
Hence, 
\begin{equation}
				\frac{\partial \mathscr{L}^{*} }{\partial w_1 } = 
				\frac{\partial C^{*} }{\partial w_1 } = x_1^{*}.
\end{equation}


In general, 
\begin{equation}
				\frac{\partial \mathscr{L}^{*} }{\partial w_{i} } =
				\frac{\partial C^{*} }{\partial w_{i} } = x_{i}^{*} = x_{i}^{H}
\end{equation}

















\end{document}

